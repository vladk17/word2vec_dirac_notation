Here I am trying to build my understanding of Language Modeling based on Word2Vec algorithm.<br>
I learn from  
* Yoav Goldberg's book "Neural Network Methods for Natural Language Processing",
* [Stanford NLP course (cs224n)](http://web.stanford.edu/class/cs224n/) ([Lecture 2](https://www.youtube.com/watch?v=ERibwqs9p38))
* Original Papers by Mikolov et al. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781), and [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
* Implementation Source code (C, Linux) from ([google archive](https://code.google.com/archive/p/word2vec/))
* [fast.ai](http://www.fast.ai/) library
* [McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
* [McCormick, C. (2017, January 11). Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)
* [Word2Vec Resources collection by McCormick](http://mccormickml.com/2016/04/27/word2vec-resources/#implementations); McCormick extensively annotated the google's [word2vec package](https://github.com/chrisjmccormick/word2vec_commented/), which is a huge help 
* "Sequence Models" as a part of ["Deep Learning Specialization"](https://www.coursera.org/specializations/deep-learning?) in Coursera
* ["On word embeddings - Part 1" by Sebastian Ruder](http://ruder.io/word-embeddings-1/index.html)
* ["On word embeddings - Part 2: Approximating the Softmax" by Sebastian Ruder](http://ruder.io/word-embeddings-softmax/)

Latex rendering does not work in md file, so I put the text into the jupyter notebook