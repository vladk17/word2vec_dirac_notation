{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec - Dirac notation\n",
    "\n",
    "<font color='red'>STATUS - VERY PRELIMINARY DRAFT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am trying to build my understanding of Language Modeling based on Word2Vec algorithm.<br>\n",
    "I learn from  \n",
    "* Yoav Goldberg's book \"Neural Network Methods for Natural Language Processing\",\n",
    "* Stanford NLP course ([Lecture 2](https://www.youtube.com/watch?v=ERibwqs9p38))\n",
    "* Original Papers by Mikolov\n",
    "* Implementation Source code (from google)\n",
    "* [fast.ai](http://www.fast.ai/) library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement - The Language Modeling Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling is the task of assigning probabilities to word sequences (sentences) in a language, or assigning probability of a given word (or sequence of words) to follow a sequence of words.<br>\n",
    "\n",
    "NLP tasks (translation, transcription, etc.) handling systems usually produce several hypotheses. These hypotheses are scored according to their probabilities (language model), and the hypotheses having the highest score is selected. <br>\n",
    "\n",
    "Language modeling is also frequently used for unsupervised pre-training. In cases when we do not have large enough amount of annotated data for our specific task, we use the language modeling, that can be trained on huge amount of unannotated text, to extract useful features that we then use while training for the our specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of a given sequence of words $w_{1:n}$ can be factorized based on probability chain-rule\n",
    "\n",
    "$$P(w_{1:n})=P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1:2})P(w_{4}|w_{1:3})...P(w_{n}|w_{1:n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resonable assumption is to say that the next word in a sequence depends only on the last $k$ words, but not on entire sequence of the previous words ($k$th order Markov assumption):\n",
    "$$P(w_{i+1}|w_{1:i})\\approx{P(w_{i+1}|w_{i-k:i})}$$\n",
    "\n",
    "Our chain of probabilities is further simplified:\n",
    "$$P(w_{1:n})=P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1:2})P(w_{4}|w_{1:3})...\\prod_{i>k}^n{P(w_{i}|w_{i-k:i-1})}$$\n",
    "\n",
    "Then, our task is to estimate $P(w_{i}|w_{i-k:i-1})$ given big long text. Practically we use counts instead of probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect a big long text from all possible sources (like wikipedia, etc.)<br>\n",
    "The order of words in this text is not random - the words are sequenced in a way that creates a meaning. <br>\n",
    "If $p(x_{k},x_{k-1},...)$ represents a probability to find words $x_{k},x_{k-1},...$ in a given order, this value should have a maximum for the meaningfull sequence of words. Probability to find a \"random\" sequence of words in a real text is very small. \n",
    "\n",
    "\n",
    "The objective is to maximize (over our big long text) the objective function\n",
    "$$J^{'}(\\theta)=\\prod_{t=1}^{T}\\prod_{\\underset{{j}\\neq{o}}{-m<=j<=m}}p(w_{t+j}|w_{t};\\theta)$$ \n",
    "In other words the probabilities that maximize the $J^{'}(\\theta)$ over the long text should represent the correct probabilistic relations between the words in this text\n",
    "\n",
    "\n",
    "After taking a $log$ and negating the sign, our objective becomes to minimize the negative $log$ likelihood\n",
    "$$J(\\theta)=-\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\underset{{j}\\neq{o}}{-m<=j<=m}}log({p(w_{t+j}|w_{t};\\theta))}$$\n",
    "$\\theta$ is a vector representation of the words (embedding vectors); $T$ all possible positions of a central word (so, $\\frac{1}{T}$ is a \"per word\" normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability to find a word $o$ as a context (in vicinity) of a center word $c$ is defined as \n",
    "$$p(o|c)=\\frac{exp(U_{o}^{T}V_{c})}{\\sum_{w=1}^{V}exp(U_{w}^{T}V_{c})}$$\n",
    "> question: may be considering the $p(o|c)$ as a transition probability from $c$ to $o$ can be more productive?\n",
    "\n",
    "<br>\n",
    "we take a big long amount of text, we go through each position in the text, for each position of the text we look at a window of size $2m$, we calculate a probability distribution of a word appearing in a context of a central word \n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirac Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing of a word by a vector and calculating mutual probabilities based on some kind of vectors product is similar to what we do in Quantum Mechanics (QM). I will try to understand how far can we take this similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first postulate of QM: physical state of a system is represented by a vector Hilbert space<br>\n",
    "Hilbert space: normalized square integralable functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirac notation - symbols to represent vectors, inner products and operators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ket: $|\\Psi>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bra: $<\\Psi|$ - some form of a conjugate to the Ket vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Braket : $<\\Psi|\\Phi>$ inner product within Hilbert space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d$-dimensional vectors for $V$ many words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of Distributed Representations has been introduced to Neural Networks community by [Hinton et al. ](https://dl.acm.org/citation.cfm?id=104287). They proposed to represent each entity by a vector. The meaning of the entity and its relations to other entities are derived from similarities between corresponding the vectors. Compare it with the first postulate of QM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Distributional hypothesis says: words that appear in similar contexts have similar meanings. \n",
    "> Word2Vec assigned similar vectors to the words that appear in similar contexts.\n",
    "> Therefore words that have similar word2vec vectors have similar meanings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
