{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec - Dirac notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First postulate of QM: physical state of a system is represented by a vector Hilbert space<br>\n",
    "Hilbert space: normalized square integralable functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirac notation - symbols to represent vectors, inner products and operators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ket: $|\\Psi>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bra: $<\\Psi|$ - some form of a conjugate to the Ket vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Braket : $<\\Psi|\\Phi>$ inner product within Hilbert space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d$-dimensional vectors for $V$ many words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability to find a word $o$ as a context (in vicinity) of a center word $c$ is defined as \n",
    "$$p(o|c)=\\frac{exp(U_{o}^{T}V_{c})}{\\sum_{w=1}^{V}exp(U_{w}^{T}V_{c})}$$\n",
    "> question: may be considering the $p(o|c)$ as a transition probability from $c$ to $o$ can be more productive?\n",
    "\n",
    "<br>\n",
    "we take a big long amount of text, we go through each position in the text, for each position of the text we look at a window of size $2m$, we calculate a probability distribution of a word appearing in a context of a central word \n",
    "<br>\n",
    "\n",
    "The objective function is to maximize (over our big long text) the objective function\n",
    "$$J^{'}(\\theta)=\\prod_{t=1}^{T}\\prod_{\\underset{{j}\\neq{o}}{-m<=j<=m}}p(w_{t+j}|w_{t};\\theta)$$ \n",
    "In other words the probabilities that maximize the $J^{'}(\\theta)$ over the long text should represent the correct probabilistic relations between the words in this text\n",
    "\n",
    "\n",
    "After taking a $log$ and negating the sign, our objective becomes to minimize the negative $log$ likelihood\n",
    "$$J(\\theta)=-\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\underset{{j}\\neq{o}}{-m<=j<=m}}log({p(w_{t+j}|w_{t};\\theta))}$$\n",
    "$\\theta$ is a vector representation of the words; $T$ all possible positions of a central word (so, $\\frac{1}{T}$ is a \"per word\" normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
